---
title: "Transformers: from self-attention to performance optimizations"
date: 2023-05-04T22:00+08:00
draft: true
---

The purpose of this post is to focus on understanding what is under the hood and the performance factors involved when fine-tuning and running local Transformer models, keeping multi-modality in mind.

To accomplish this, we first present a brief account of the transformer architecture, including its design intuitions and the underlying mathematics, concretized by illustrative diagrams and code snippets. Then we aim to achieve a comprehensive understanding of the widely adopted performance optimizations for the original transformer architecture.

This post was initially inspired by [An Introduction to Transformers (Turner, 2023)](https://arxiv.org/abs/2304.10557) [^1], which provides a mathematically precise, intuitive, and clean description of the transformer architecture. Many other sources are incorporated and improved upon for clarity and self-containedness.

The illustrative diagrams in this post aims to be reproduceable. They are generated by either the LaTeX subset supported by [MathJax](https://www.mathjax.org/), or [D2](https://d2lang.com/), a modern diagram scripting language.

## Notations

Notations used in this post try to be consistent with "The Transformer Family Version 2.0" (Lilian, 2023)[^3] and latest papers on performance optimizations such as FlashAttention, GPTQ, etc. , which deviate from (Turner, 2023)[^1] and (Phuong et al., 2022)[^2] in many places, notably the shape of input/output matrices.

For further simplicity, this post will use a simplified version of the concise and powerful "Named Tensor Notation" from (Chiang et al., 2021)[^7] which has many advantages similar to Einops (Rogozhnikov, 2022)[^8]. It's seldom used in the literatures but very intuitive and easy to recover the widely adopted notations visually. Essentially, it's a notation trying to avoid complicated indices, by giving indices a name and only specify the indices involved in the operation (e.g. summations,reductions, contractions, reshaping, etc.).

$$
\newcommand{\namedtensorstrut}{\vphantom{fg}} % milder than \mathstrut
\newcommand{\name}[1]{\mathsf{\namedtensorstrut #1}}
\newcommand{\nbin}[2]{\mathbin{\underset{\substack{#1}}{\namedtensorstrut #2}}}
\newcommand{\ndot}[1]{\nbin{#1}{\odot}}
\newcommand{\ncat}[1]{\nbin{#1}{\oplus}}
\newcommand{\nsum}[1]{\sum\limits_{\substack{#1}}}
\newcommand{\nfun}[2]{\mathop{\underset{\substack{#1}}{\namedtensorstrut\mathrm{#2}}}}
\newcommand{\ndef}[2]{\newcommand{#1}{\name{#2}}}
\ndef{\ax}{ax}
\ndef{\dd}{d}
\ndef{\layer}{layer}
\ndef{\seq}{seq}
\ndef{\subseq}{subseq}
\ndef{\key}{key} \ndef{\val}{val}
\ndef{\heads}{heads}
\ndef{\batch}{batch}
\ndef{\inp}{input} \ndef{\hidden}{hidden} \ndef{\out}{out}
\ndef{\height}{height} \ndef{\width}{width} \ndef{\chans}{chans}
\ndef{\kernel}{kernel} \ndef{\kh}{kh} \ndef{\kw}{kw}
\ndef{\vocab}{vocab}
\ndef{\classes}{classes}
\ndef{\state}{state}
\ndef{\emb}{emb}
\ndef{\feat}{feat}
\def\attnbox#1{\colorbox{#1}{\color{#1} O}}
\def\smtxt#1{\text{\small{#1}}}
$$

| Symbol | $\in$ | Meaning |
| :---: | :---: | :--- |
| $X^{(0)}$ | $\mathbb{R}^{\seq \times \feat}$ | The input sequence of tokens, length $N$, each token has $D$ features |
| $X^{(m)}$ | $\mathbb{R}^{\seq \times \feat}$ | The output of the $m^{th}$ transformer layer |
| $x^{(m)}_n $ | $\mathbb{R}^{1 \times \feat}$ | The (row) vector of features for the $n^{th}$ token of $X^{(m)}$ |
| $A^{(m)}$ | $\mathbb{R}^{\seq \times \seq}$ | The $N \times N$ attention matrix |

To convey the types/shapes and the definition of operations simontaneously, sometimes this post will follow [Lean](https://leanprover.github.io/) syntax to specify type where necessary, it looks like this:

$ f : A → B → C := do(a, b) $

where

- $f$ is an operation accepting two operands of type $A$ and $B$ respectively, and returns a value of type $C$
- $f$ is defined as applying some operation $do$ to $a$ and $b$ where $do$ can be arbitrary operation like $a + b$ 
- $:$ is read as "has type", $:=$ is read as "is defined as", and $→$ is read as "maps to".

When the types of operands are clear, we could also write the above as:

$ f = do(a, b) \quad : C$

$$
% NxD matrices in diargams are assumed to be 4x3, i.e. 4 rows and 3 columns
% an example element would be placed on [2, 1], i.e. the 3rd row and the middle column
% a row vector, size 3, with the symbol placed at the center
\def\rvec#1{{\fbox{$ \rule{2em}{0pt} {#1} \rule{2em}{0pt} $}}}
% a wide row vector, size 4, with the symbol placed at the center
\def\wrvec#1{{\fbox{$ \rule{3em}{0pt} {#1} \rule{3em}{0pt} $}}}
% a column vector, size 4, with the symbol placed at the center
\def\cvec#1{\fbox{$ {#1} \rule[-3em]{0pt}{6.5em} $}}
$$

## The input/output of a transformer

A transformer is capable of ingesting "a sequence of tokens" and generating "a representation of the sequence". 

Here, a token refers to a $D$-dimensional vector that represents a small unit of data, e.g. a (sub-)word in a sentence, or an 8x8 patch of a 512x512 image. These units are not fixed and can be chosen based on some domain-specific insight into the underlying structure of the data type.

Representing a token as a vector is commonly called an embedding, and usually is learned to better express the token's features and the relationships among tokens. Each dimension of a embedding vector is called a feature but it's not nessarily a feature in the traditional sense such as an attribute of an object.

Turning data into "a sequence of tokens" is a process called tokenization. After tokenization, the input becomes a sequence of $L$ tokens of dimension $D$, thus can be collected into a $N \times D$  matrix:

$$
\def\Xinput{\begin{bmatrix}
\vdots \cr
\vdots \cr
\rvec{x_n^{(0)}} \cr
\vdots \cr
\end{bmatrix}}
X^{(0)} = \underbrace{\Xinput}_{D \text{ columns (features) }}\left.\vphantom{\cvec{}}\right\\}N \text{ rows (tokens)}
$$

where $X^{(0)}$ denotes the input of the transformer, and $x_n^{(0)}$ is the (row) vector of features for the $n^{th}$ token.

The output, "a representation of the sequence", takes the same form as "a sequence of tokens", and the $n^{th}$ "token" is a $D$-dimensional vector representing the sequence at the location of token $n$.

Various tasks can be achieved by designing an appropriate representation, e.g. auto-regressive prediction of the next $(n+1)^{th}$ token, global classification of the entire sequence (by pooling across the whole representation), sequence-to-sequence or image-to-image prediction problems, etc.

The input/output of a transformer is a powerful and versatile abstraction so that it can be used for mixing data of different modalities (images, texts etc.) and mixed tasks of different types.

For texts, the tokenization process first needs to choose a vocabulary that could cover almost all the words in the language, plus some meta-tokens like `bos_token` representing the beginning of sequence, `eos_token` representing the end of sequence, and `mask_token` for masked language modelling. Then it needs to learn the embedding of each vocabulary element, and it would be clear in a bit that it also needs to learn the embedding of the position of each token in the sequence, called positional embedding, which I presume to be a geometric embedding in general. Latest researches, such as ALiBi (Press et al., 2022 )[^6], suggest that positional information can be encoded in forms other than positional embedding.

(TODO: refer to subword tokenization literatures such as Byte Pair Encoding used in GPT-2, and cover other commonly used tokenization methods such as SentencePiece, as well as various ways to do positional embedding.)

(TODO: explain commonly used tokenizers in popular models, particularly the treatment for non-English languages, but better as an appendix)

## The transformer layer

A transformer is composed of multiple transformer layers. The representation of the input sequence will be produced by iteratively applying a transformer layer

$$ X^{(m)} = \text{TransLayer}(X^{(m-1)}) $$

where $X^{(m)}$ denotes the output of the $m^{th}$ transformer layer, and recall that $X^{(0)}$ naturally denotes the input of the transformer.

Every transformer layer comprises two stages (or sub-layers):

* Stage 1 refines each feature across the whole sequence
* Stage 2 refines each token

By repeatedly applying the transformer layer the representation at
token $n$ and feature $d$ can be shaped by information at token $n'$ and feature $d'$ . This gives the transformer the ability to model long-range dependencies between tokens and features. Such a completeness is a key advantage of the transformer over other architectures but also poses challenges for efficient implementation, especially for long sequences.

### Stage 1: self-attention across the sequence

Stage 1 produces another $N \times D$ matrix, denoted $Y^{(m)}$. For simplicity, we'll omit the superscripts $(m)$ where it's clear from the context.

Stage 1 is based on the self-attention mechanism, which is a special case of the attention mechanism. There're many types of attentions, see ["A Family of Attention Mechanisms"](https://lilianweng.github.io/posts/2018-06-24-attention/#a-family-of-attention-mechanisms) for a summary.

#### Attention

The key idea of the attention mechanism is to infer by focusing on a given set of data, hence the name "Attention", as in the famous quote "Attention is all you need" from (Vaswani et al., 2017)[^4]. It's initially introduced by (Bahdanau et al., 2015)[^5] for machine translation where the attention mechanism is used to focus on the source sentence when translating a target word.

For visual tasks, the attention mechanism is often used to focus on a small region or some closely related regions of an image. For text tasks, it could be used to focus on the relationship between words in one sentence or close context. For multi-modal tasks, it could relate within a modality or between modalities.

The amount of attention is quantified by learned weights given by a so-called attention matrix $A \in \mathbb{R}^{\seq \times \seq}$. 

The output is usually formed as a weighted average, compactly written as a matrix product:


$$
Y^{(m)} = A^{(m)} X^{(m-1)}
= A^{(m)} \underset{\seq}{\odot} X^{(m-1)}
= \sum_{\seq} A^{(m)} \odot X^{(m-1)}
$$

which is essentially doing the following for each feature $d$:

$$
\def\Ynd{\begin{bmatrix}
 & \vdots &  \cr
 & \vdots &  \cr
\cdots & Y_{n, d} & \cdots \cr
 & \vdots & 
\end{bmatrix}}
\def\Xod{\begin{bmatrix}
\cdots & \cvec{X_{:, d}} & \cdots 
\end{bmatrix}}
\def\Ano{\begin{bmatrix}
\vdots \cr
\vdots \cr
\wrvec{A_{n, :}} \cr
\vdots
\end{bmatrix}}
\Ynd_{N \times D} = \Ano_{N \times N} \times \Xod_{N \times D}
$$

Here $A$  normalizes over each column $\sum\limits_{\seq} A =1$.

Specifically, $A_{n, n^{\prime}}$ will take a high value for locations in the sequence $n^{\prime}$ which are of high relevance for location $n$, where $n^{\prime}$ denotes a location in the slice $X_{:, d}$.

The following example of attention matrix demonstrates translating from the English sentence "eating a green apple" to the French sentence "manger une pomme verte" assuming only 1 feature for simplicity.

Note that the 3rd token ("pomme", i.e. "apple") in the French sentence pays the greatest attention (marked by a black box) to the 4th token "apple" in the English sentence, and also attend to the 1st token "eating" and the 3rd token "green" in the English sentence (marked by grey boxes) because they're related context to "apple" but almost none to the 2nd token "a" (marked by a lighter grey box).

$$
\def\Ynd{\begin{bmatrix}
\text{manger}  \cr
\text{une}  \cr
\fbox{pomme} \cr
\text{verte}
\end{bmatrix}}
\def\Xod{\begin{bmatrix}
& \fbox{ $ \begin{array}{c}
    \text{eating} \newline \text{a} \newline \text{green} \newline \text{apple}
    \end{array} $ } &
\end{bmatrix}}
\def\Ano{\begin{bmatrix}
\vdots \cr
\vdots \cr
\fbox{ \attnbox{gray} \attnbox{darkgray} \attnbox{gray} \attnbox{black} } \cr
\vdots
\end{bmatrix}}
\Ynd_{4 \times 1} = \Ano_{4 \times 4} \times \Xod_{4 \times 1}
$$

#### Self-attention

**Self-attention**, also known as intra-attention, first introduced by (Cheng et al., 2016)[^9], has been shown to be very useful in machine reading, abstractive summarization, or image description generation.

It's a special case of the attention mechanism where the attention matrix relate between different locations of the same input sequence, that is, to make prediction for one part of the data using other parts of the observation about the same data.

The attention matrix in the example above becomes:

$$
\begin{array}{c}\smtxt{eating} \cr \smtxt{a} \cr \smtxt{green} \cr  \smtxt{apple} \end{array}
\overset{\begin{array}{cccc}\smtxt{eating} & \smtxt{a} & \smtxt{green} &  \smtxt{apple} \end{array}}{
\begin{bmatrix}
\attnbox{black} \attnbox{lightgray} \attnbox{lightgray} \attnbox{gray} \cr
\attnbox{lightgray} \attnbox{black} \attnbox{darkgray} \attnbox{lightgray} \cr
\attnbox{lightgray} \attnbox{lightgray} \attnbox{black} \attnbox{darkgray} \cr 
\attnbox{gray} \attnbox{darkgray} \attnbox{gray} \attnbox{black} 
\end{bmatrix}
}
$$

Note that this matrix is asymmetric:

- Semantics related:
  - "apple" pays more attentions to "green" (which is describing it) than the other way around because "green" is a general color regardless what it's describing
- Grammar related:
  - "apple" pays unilateral attention to "a" for its quantity
  - "a" pays unilateral attention to "green" for whether it should be "an" instead

A simple way of generating the attention matrix from the input would be to measure the similarity between two locations by the dot product between the features at those two locations and then use a softmax function to handle the normalisation i.e.

$$
A = \nfun{\seq}{softmax}(X \ndot{\feat } X)
$$

where

$$
\nfun{\seq}{softmax} X =\frac{\exp X}{\nsum{\seq} \exp X}
$$

However, this naïve approach is only about the similarity between the content of the sequence at different locations, not the relation between the locations. 

An improvement is to perform the same operation on a linear transformation of the sequence, $U : \mathbb{R}^{\seq \times \feat} \to \mathbb{R}^{\seq \times \feat} := U \ndot{\seq} X $, so that 

$$
\newcommand{\namedtensorstrut}{\vphantom{fg}} % milder than \mathstrut
\newcommand{\nbin}[2]{\mathbin{\underset{\substack{#1}}{\namedtensorstrut #2}}}
\newcommand{\ndot}[1]{\nbin{#1}{\odot}}
\newcommand{\nsum}[1]{\sum\limits_{\substack{#1}}}
\newcommand{\nfun}[2]{\mathop{\underset{\substack{#1}}{\namedtensorstrut\mathrm{#2}}}}
\newcommand{\ndef}[2]{\newcommand{#1}{\name{#2}}}
A = \nfun{\seq}{softmax} \left( (U \ndot{\seq} X) \ndot{\feat} ( U \ndot{\seq} X ) \right)
$$

In this way only some of the features in the input sequence need be used to compute the similarity, the others being projected out, thereby decoupling the attention computation from the content.

However, this attention matrix is symmetric unlike the asymmetric version we need to express asymmetric relations. The solution is to use two different linear transformations to compute the similarity, i.e.

$$
\newcommand{\namedtensorstrut}{\vphantom{fg}} % milder than \mathstrut
\newcommand{\nbin}[2]{\mathbin{\underset{\substack{#1}}{\namedtensorstrut #2}}}
\newcommand{\ndot}[1]{\nbin{#1}{\odot}}
\newcommand{\nsum}[1]{\sum\limits_{\substack{#1}}}
\newcommand{\nfun}[2]{\mathop{\underset{\substack{#1}}{\namedtensorstrut\mathrm{#2}}}}
\newcommand{\ndef}[2]{\newcommand{#1}{\name{#2}}}
A = \nfun{\seq}{softmax} ( Q \ndot{\feat} K ) = \nfun{\seq}{softmax} \left( (W_q \ndot{\seq} X) \ndot{\feat} ( W_k \ndot{\seq} X ) \right)
$$

Here we switch to using $W$ instead of $U$ to denote the linear transformations because they're the learnable **w**eights, and using the subscripts $q$ and $k$ because these weights are known as the queries and keys in [^4], and $Q$ and $K$ are called query matrix and key matrix, respectively.

### Stage 2: multi-layer perceptron across features

## References

[^1]: Richard E. Turner, ["An Introduction to Transformers"](https://arxiv.org/abs/2304.10557), arXiv:2304.10557 (2023)

[^2]: Mary Phuong and Marcus Hutter, ["Formal algorithms for transformers"](https://arxiv.org/abs/2207.09238), arXiv:2207.09238 (2022)

[^3]: Lilian Weng, ["The Transformer Family Version 2.0"](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/), Lil’Log (Jan 2023)

[^4]: Ashish Vaswani, et al. ["Attention is all you need."](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), NIPS 2017.

[^5]: Dzmitry Bahdanau, et al. ["Neural machine translation by jointly learning to align and translate"](https://arxiv.org/abs/1409.0473), ICLR 2015.

[^6]: Press et al. ["Train Short, Test Long: Attention With Linear Biases Enables Input Length Extrapolation."](https://arxiv.org/abs/2108.12409) ICLR 2022.

[^7]: Chiang et al. ["Named Tensor Notation"](https://arxiv.org/abs/2102.13196), arXiv:2102.13196 (2021)

[^8]: Alex Rogozhnikov, ["Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation"](https://openreview.net/forum?id=oapKSVM2bcj), ICLR 2022. 

[^9]: Jianpeng Cheng et al. ["Long short-term memory-networks for machine reading"](https://arxiv.org/abs/1601.06733) arXiv preprint arXiv:1601.06733 EMNLP 2016.